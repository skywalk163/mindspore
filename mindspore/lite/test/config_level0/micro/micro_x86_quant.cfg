[common_quant_param]
# Supports WEIGHT_QUANT or FULL_QUANT
quant_type=FULL_QUANT
# Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization
# Full quantization support the number of bits [1,8]
bit_num=8
# Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.
#min_quant_weight_size=0
# Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.
#min_quant_weight_channel=16

[data_preprocess_param]

calibrate_path=

calibrate_size=1

input_type=BIN

[full_quant_param]

activation_quant_method=MAX_MIN

bias_correction=false

target_device=DSP

[micro_param]
# enable code-generation for MCU HW 
enable_micro=true

# specify HW target, support x86,Cortex-M, ARM32, ARM64 only.
target=x86

# code generation for Inference or Train
codegen_mode=Inference

# enable parallel inference or not
support_parallel=false

# enable debug
debug_mode=false

[common_quant_param]
# Supports WEIGHT_QUANT or FULL_QUANT
#quant_type=WEIGHT_QUANT
# Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization
# Full quantization support the number of bits [1,8]
#bit_num=8
# Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.
#min_quant_weight_size=0
# Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.
#min_quant_weight_channel=16


[micro_param]
# enable code-generation for MCU HW 
enable_micro=true

# specify HW target, support x86,Cortex-M, ARM32, ARM64 only.
target=ARM64

# code generation for Inference or Train
codegen_mode=Inference

# enable parallel inference or not
support_parallel=false

# enable debug
debug_mode=false

# false indicates that only the required weights will be saved. Default is false.
# If collaborate with lite-train, the parameter must be true.
keep_original_weight=false

# the names of those weight-tensors whose shape is changeable, only embedding-table supports change now.
# the parameter is used to collaborate with lite-train. If set, `keep_original_weight` must be true.
#changeable_weights_name=name0,name1